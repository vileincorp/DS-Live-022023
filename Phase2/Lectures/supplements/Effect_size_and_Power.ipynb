{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Effect Size and Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "    # statsmodels also has FTestPower and FTestAnovaPower classes!\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "SWBAT:\n",
    "\n",
    "- Visualize and explain effect size in Python\n",
    "- Conduct power analysis in Python\n",
    "- Explain what power and power analysis are in context  \n",
    "- Explain type I and type II error in specific context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1. Effect Size\n",
    "\n",
    "Effect size is used to quantify the *size* of the difference between two groups under observation. Effect sizes are easy to calculate, understand and apply to any measured outcome and are applicable to a multitude of study domains.\n",
    "\n",
    "We know already that $p$-values tell us (together with some $\\alpha$-threshold) *whether* differences between groups are significant; the idea now is to have a measure of *how* significant differences are.\n",
    "\n",
    "### Why do we need $p$-values *and* effect size?\n",
    "\n",
    "[Here](https://www.physport.org/recommendations/Entry.cfm?ID=93385) is a helpful resource. The basic idea is this:\n",
    "\n",
    "We can shrink $p$-values rather artificially by increasing our sample size, no matter how small the effect size between two groups. So, with a large enough sample size, we can have as small a $p$-value as we like––but it won't be an interesting result if the effect size is small.\n",
    "\n",
    "The opposite difficulty is possible as well: Our $p$-value can be quite large if our sample size is small enough. And that's true even if the effect size is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Compare effect size of gender in height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Seed the random number generator so we all get the same results\n",
    "np.random.seed(10)\n",
    "\n",
    "# Mean height and sd for males\n",
    "male_mean = 178\n",
    "male_sd = 7.7\n",
    "\n",
    "# Generate a normal distribution for male heights \n",
    "male_height = stats.norm(male_mean, male_sd)\n",
    "\n",
    "female_mean = 163\n",
    "female_sd = 7.3\n",
    "female_height = stats.norm(female_mean, female_sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_height.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_PDF(rv, x=4):\n",
    "    \"\"\"\n",
    "    Input: a random variable object, number of standard deviations\n",
    "    Output: x and y values for the normal distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify the mean and standard deviation of random variable.\n",
    "    mean = rv.mean()\n",
    "    std = rv.std()\n",
    "\n",
    "    # Use numpy to calculate evenly spaced numbers over the\n",
    "    # specified interval (4 sd by default) and generate 100 samples.\n",
    "    xs = np.linspace(mean - x*std, mean + x*std, 100)\n",
    "    \n",
    "    # Calculate the normal distribution i.e. the probability density. \n",
    "    ys = rv.pdf(xs)\n",
    "\n",
    "    return xs, ys # Return calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Male height\n",
    "xs, ys = evaluate_PDF(male_height)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xs, ys, label='male', linewidth=4, color='#beaed4') \n",
    "\n",
    "#Female height \n",
    "xs, ys = evaluate_PDF(female_height)\n",
    "ax.plot(xs, ys, label='female', linewidth=4, color='#fdc086')\n",
    "\n",
    "ax.set_xlabel('height (cm)')\n",
    "ax.set_ylabel('probability density')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cohen's $d$, standardized metrics for effect size\n",
    "Cohen’s $d$ is one of the most common ways to measure effect size. As an effect size, Cohen's $d$ is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable.\n",
    "\n",
    "$d$ = difference of means / pooled standard deviation;\n",
    "\n",
    "$d = \\frac{\\mu_1 - \\mu_2}{\\sigma_{pooled}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def Cohen_d(group1, group2):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes Cohen's d.\n",
    "    \"\"\"\n",
    "    \n",
    "    # group1: Series or NumPy array\n",
    "    # group2: Series or NumPy array\n",
    "\n",
    "    # returns a floating point number \n",
    "\n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1 = len(group1)\n",
    "    n2 = len(group2)\n",
    "    var1 = group1.var(ddof=1)\n",
    "    var2 = group2.var(ddof=1)\n",
    "\n",
    "    # Calculate the pooled variance\n",
    "    pooled_var = ((n1-1) * var1 + (n2-1) * var2) / (n1 + n2 - 2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "# x1 = np.random.normal(male_mean, male_sd, 1000)\n",
    "# x2 = np.random.normal(female_mean, female_sd, 1000)\n",
    "\n",
    "female_sample = female_height.rvs(1000)\n",
    "male_sample = male_height.rvs(1000)\n",
    "\n",
    "effect = Cohen_d(male_sample, female_sample)\n",
    "print(effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluating Effect Size\n",
    "\n",
    "[good demo here](https://rpsychologist.com/cohend/)\n",
    "\n",
    "Small effect = 0.2\n",
    "\n",
    "Medium Effect = 0.5\n",
    "\n",
    "Large Effect = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pdfs(cohen_d=2):\n",
    "    \"\"\"\n",
    "    Plot PDFs for distributions that differ by some number of stds.\n",
    "    cohen_d: number of standard deviations between the means\n",
    "    \"\"\"\n",
    "    group1 = stats.norm(0, 1)\n",
    "    group2 = stats.norm(cohen_d, 1)\n",
    "    xs, ys = evaluate_PDF(group1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.fill_between(xs, ys, label='Female', color='#ff2289', alpha=0.7)\n",
    "\n",
    "    xs, ys = evaluate_PDF(group2)\n",
    "    ax.fill_between(xs, ys, label='Male', color='#376cb0', alpha=0.7)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2. Error\n",
    "When conducting hypothesis testing, we __choose__ a value for alpha, which represents the risk of falsely rejecting the null hypothesis. If, as is somewhat conventional, we set the alpha at 0.05, then we are saying that \"for 5% of the time, we are willing to reject the null hypothesis when it is in fact true\". How, then, do we categorize different types of error associated with conducting the experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Type I error\n",
    "Type I error is usually represented as $\\alpha$, which is the probability of rejecting the null hypothesis when it is in fact true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Type II Error\n",
    "Type II error is represented as $\\beta$; it is the probability of failing to reject the null hypothesis when it is in fact false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3. Power\n",
    "How does statistical power relate to two types of error? Power is defined as the __probability of not making a Type\n",
    "II error__ (i.e., probability of correctly rejecting H0 when it is in fact false)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Relevance of Power?\n",
    "- Low Statistical Power: Large risk of committing Type II errors, i.e. a false negative.\n",
    "- High Statistical Power: Small risk of committing Type II errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Elements that affect power:\n",
    "- Effect Size\n",
    "- Sample Size (and thus Standard Error)\n",
    "- Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does sample size affect power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four quantities––$\\alpha$, sample size, effect size, and power––have important interrelationships for hypothesis testing that this notebook will illustrate.\n",
    "\n",
    "Let's recall what each of these is and why each is important to experimental design:\n",
    "\n",
    "- $\\alpha$ is our false-positive rate, i.e the rate at which our tests will lead us to reject the null hypothesis when in fact it is true. This is the same $\\alpha$ that determines the size of our test statistic (which in turn affects the size of our confidence intervals). This is semi-standardly set to 0.05, but in practice an appropriate value will depend on the nature of the tests. How costly would a false positive be?\n",
    "\n",
    "- Sample Size, often indicated with $n$, is of course just how many points we have in our sample. Note that we may or may not have much control over this! One common scenario is to calculate how large your sample size needs to be in order to have a test that has a given degree of power.\n",
    "\n",
    "- Effect Size, commonly measured by Cohen's $d$ statistic, is a parameter over which one has effectively **no control**. This is a measure of *how different* two samples are, and so this is of course a reflection of underlying reality, as opposed to a result of experimental choice.\n",
    "\n",
    "- Power is $1 - \\beta$, where $\\beta$ is our false-negative rate. Since $\\beta$ tells us the rate at which our tests will lead us *to fail* to reject the null hypothesis when in fact it is false, the power of a test thus tells us the rate at which our tests lead us to reject the null hypothesis when it is false. Of course, the null hypothesis is false when we have some genuine difference between two samples, and so power is a measure of the ability of a test to pick up on those differences when they are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_default = 0.05\n",
    "n_default = 100\n",
    "d_default = 0.5\n",
    "pow_default = 0.8\n",
    "\n",
    "test = TTestIndPower()\n",
    "\n",
    "## Power as a function of effect size\n",
    "ds = np.linspace(0, 1, 100)\n",
    "power_per_d = []\n",
    "for d in ds:\n",
    "    power_per_d.append(test.solve_power(alpha=alpha_default,\n",
    "                                        nobs1=n_default, effect_size=d))\n",
    "\n",
    "## Power as a function of sample size\n",
    "ns = np.arange(1, 100, 1)\n",
    "power_per_n = []\n",
    "for n in ns:\n",
    "    power_per_n.append(test.solve_power(alpha=alpha_default,\n",
    "                                        nobs1=n, effect_size=d_default))\n",
    "\n",
    "## Power as a function of alpha\n",
    "alphas = np.linspace(0.01, 0.1, 100)\n",
    "power_per_alpha = []\n",
    "for alpha in alphas:\n",
    "    power_per_alpha.append(test.solve_power(alpha=alpha,\n",
    "                                            nobs1=n_default, effect_size=d_default))\n",
    "\n",
    "\n",
    "## Sample size as a function of alpha\n",
    "n_per_alpha = []\n",
    "for alpha in alphas:\n",
    "    n_per_alpha.append(test.solve_power(alpha=alpha,\n",
    "                                        effect_size=d_default, power=pow_default))\n",
    "\n",
    "## Plotting\n",
    "fig, ax = plt.subplots(4, 1, figsize=(10, 20))\n",
    "ax[0].plot(ds, power_per_d)\n",
    "ax[0].set_xlabel('Cohen\\'s d')\n",
    "ax[0].set_ylabel('Power')\n",
    "ax[0].set_title('The greater the effect size, the more powerful my test!')\n",
    "ax[1].plot(ns, power_per_n)\n",
    "ax[1].set_xlabel('Sample Size')\n",
    "ax[1].set_ylabel('Power')\n",
    "ax[1].set_title('The larger the sample size, the more powerful my test!')\n",
    "ax[2].plot(alphas, power_per_alpha)\n",
    "ax[2].set_xlabel('alpha')\n",
    "ax[2].set_ylabel('Power')\n",
    "ax[2].set_title('The larger the false positive rate, the more powerful my test!')\n",
    "ax[3].plot(alphas, n_per_alpha)\n",
    "ax[3].set_xlabel('alpha')\n",
    "ax[3].set_ylabel('Sample Size')\n",
    "ax[3].set_title('The larger the false positive rate, the smaller the sample\\\n",
    " I need for a test of a given power!')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is the formula for calculating power?\n",
    "\n",
    "Power is usually complex to calculate, but there are sometimes tables available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Examples of power tables:\n",
    "- [full power table](http://www.pilesofvariance.com/Chapter13/Cohen_Power_Tables.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Power Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.plot_power(dep_var='nobs',\n",
    "                         nobs=np.arange(10, 100),\n",
    "                         effect_size=[0.2, 0.5, 0.8, 1.3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 4. Case Study \n",
    "Suppose you are launching a pilot study with Instagram and you want to examine whether the new feature––making the heart when you \"like\" someone's photo red instead of white––that was developed by the front-end engineer attracted more likes given that other variables are being held constant. You have collected two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = pd.read_csv('data/likes_experiment.csv', index_col=0)\n",
    "control = pd.read_csv('data/likes_control.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "control.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculate effect size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = Cohen_d(experiment['Likes_Given_Exp'], control['Likes_Given_Con'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solve for sample size\n",
    "\n",
    "We can use `power_analysis.solve_power` from `statsmodels` to find the sample size you need.\n",
    "\n",
    "[documentation here](https://www.statsmodels.org/dev/generated/statsmodels.stats.power.tt_ind_solve_power.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We want to know how many observations we need in order to\n",
    "# attain a power of 0.8, given an alpha of 0.05\n",
    " \n",
    "#effect_size = 0.8\n",
    "alpha = 0.05 # significance level\n",
    "power = 0.8\n",
    "\n",
    "power_analysis = TTestIndPower()\n",
    "sample_size = power_analysis.solve_power(effect_size=effect_size, \n",
    "                                         power=power, \n",
    "                                         alpha=alpha)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(experiment.shape)\n",
    "print(control.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luckily, we do have enough observations to conduct this experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting a T Test\n",
    "stats.ttest_ind(experiment['Likes_Given_Exp'], control['Likes_Given_Con'],\n",
    "               equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#warnings.filterwarnings(\"ignore\")\n",
    "sns.kdeplot(experiment['Likes_Given_Exp'], shade=True)\n",
    "sns.kdeplot(control['Likes_Given_Con'], shade=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Word of Caution\n",
    "\n",
    "This looks like a highly significant result! The low p-value tells us that the difference in sample statistics is likely not the result of chance. But we can infer that the new Instagram feature is relevant to the difference only if we can rule out other factors. Good experimental design would require that the control and experimental groups be constructed randomly. Let's check the \"Average Likes Given\" column to make sure we're on safe ground!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size_avg = Cohen_d(experiment['Avg_Likes_Given_Exp'], control['Avg_Likes_Given_Con'])\n",
    "effect_size_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(experiment['Avg_Likes_Given_Exp'], control['Avg_Likes_Given_Con'],\n",
    "               equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Indeed, this difference is not so significant. So we have good reason to think that there is a very significant difference between the two groups that cannot be explained by a difference in average likes between the two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Limitations of Cohen's $d$\n",
    "\n",
    "Remember the story we told back in Part 1? It went like this: For a given effect size, we can shrink our $p$-value by increasing our sample size or inflate our $p$-value by decreasing our sample size. But the trouble with this story is that it assumes that effect size is independent of sample size. This, at least if we are measuring effect size in terms of Cohen's $d$, is false, since that metric depends on the standard deviation, which, in turn, depends on sample size.\n",
    "\n",
    "Of course, if we had access to the population parameters, then we could calculate Cohen's $d$ on those and this worry would effectively vanish. But in practice we do not have access to these, and so we are in the familiar position of estimating these based on samples.\n",
    "\n",
    "[Here](https://garstats.wordpress.com/2018/04/04/dbias/) is a nice resource on this problem.\n",
    "\n",
    "The basic upshot is this: Cohen's $d$, when calculated on samples, tends to **overestimate** the true effect size of a difference between populations (because population standard deviations are often larger than sample standard deviations), **especially** for small sample sizes. So: If your sample sizes are relatively small, take Cohen's $d$ with a grain of salt––or use some more robust metric of effect size.\n",
    "\n",
    "For more on this, see [here](https://www.statisticssolutions.com/statistical-analyses-effect-size/) and [here](https://garstats.wordpress.com/2016/05/02/robust-effect-sizes-for-2-independent-groups/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Free Online Power Calculator\n",
    "\n",
    "Online Power Calculator: https://clincalc.com/Stats/Power.aspx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
