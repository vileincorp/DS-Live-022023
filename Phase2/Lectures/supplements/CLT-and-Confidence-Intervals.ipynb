{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Central Limit Theorem and Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting Sampling\n",
    "\n",
    "Before we get into a theorem that's related to sampling, we should remember some key things about sampling.\n",
    "\n",
    "<img src=\"../images/sample_pop.png\" alt=\"sampling example with circled people\" width=800>\n",
    "\n",
    "Our goal when sampling is to find a representative group, from which we can infer population parameters.\n",
    "\n",
    "Remember: samples have statistics, populations have parameters.\n",
    "\n",
    "In order to do this effectively, our sample should be randomly selected and representative of our population (which sounds nice and easy, it's hard to do in practice). We are trying to minimize bias in our sample, but also we are sampling because we're trying to minimize cost as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem (CLT)\n",
    "\n",
    "Now here's something cool:\n",
    "\n",
    "> \"The central limit theorem states that the sampling distribution of the mean of any independent, random variable will be normal or nearly normal, if the sample size is large enough.\" \n",
    "- From [Stat Trek](https://stattrek.com/statistics/dictionary.aspx?definition=central_limit_theorem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example, using the popular Iris dataset:\n",
    "\n",
    "<img src=\"../images/probability-basics.gif\" width=800>\n",
    "\n",
    "Here, taking samples of 15 flowers at a time and measuring their sepal lengths, we see that the the mean of the samples is normally distributed, where the mean of that normal distribution approximates the population mean. \n",
    "\n",
    "The coolest part is - this is true and works almost no matter what the original distribution is! (I say 'almost' because there are exceptions, of course). \n",
    "\n",
    "Let's check this out in code!\n",
    "\n",
    "Data Source: https://data.seattle.gov/City-Business/City-of-Seattle-Wage-Data/2khk-5ukd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our Seattle city worker data again\n",
    "df = pd.read_csv('../data/City_of_Seattle_Wage_Data_Aug21.csv',\n",
    "                header=0, names=['Department', 'Last Name', 'First Name', 'Job Title', 'Hourly Rate'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hourly rate column is not perfectly normally distributed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Hourly Rate'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take 100 samples of 50 employees each time, and see what their average hourly rate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100 # number of samples\n",
    "n = 50 # number of employees per sample\n",
    "\n",
    "# The full for loop version:\n",
    "# sample_means = []\n",
    "# for x in range(n):\n",
    "#     sample_emps = df.sample(n=n)\n",
    "#     sample_mean = sample_emps['Hourly Rate '].mean()\n",
    "#     sample_means.append(sample_mean)\n",
    "    \n",
    "# But let's do this with list comprehension:\n",
    "sample_means = [np.mean(df.sample(n=n)) for x in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of our samples, plus our pop and sample means\n",
    "pop_mean = df['Hourly Rate'].mean()\n",
    "samp_mean = np.mean(sample_means)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "sns.distplot(sample_means, bins=20)\n",
    "\n",
    "plt.vlines(pop_mean, ymin=0, ymax=.25, color='r', linestyle=':',\n",
    "           label=f'Population Mean: {pop_mean:.2f}')\n",
    "plt.vlines(samp_mean, ymin=0, ymax=.25, color='g', linestyle=':',\n",
    "           label=f'Mean of 100 Samples: {samp_mean:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because sample statistics are imperfect representations of the true population values, it is often appropriate to state these estimates with **confidence intervals**.\n",
    "\n",
    "Before proceeding, let's talk about how to _interpret_ a confidence interval.\n",
    "\n",
    "Suppose our Indian correspondent (or David Attenborough) takes several hundred measurements of parrot beak lengths in the Ganges river basin and calculates an average beak length of 9cm. He reports this measure by saying that the 90%-confidence interval is (8.6, 9.4).\n",
    "\n",
    "This does NOT mean that we should be 90% confident that the true population mean beak length is somewhere between 8.6cm and 9.4cm. Rather, what our correspondent means is that, if we were to conduct the same measuring experiment many times, constructing intervals in the same way, **we should expect 90% of those intervals to contain the true population mean.**\n",
    "\n",
    "Again, for emphasis: \n",
    "\n",
    "> \"The **95%** in a **95% confidence interval** tells us that if we calculated a confidence interval from **100** different samples, about **95** of them would contain the **true population mean**.\"\n",
    "\n",
    "-- [Crash Course Statistic's video on Confidence Intervals](https://youtu.be/yDEvXB6ApWc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, how do we calculate them?\n",
    "\n",
    "The confidence interval we construct will depend on the statistics of our sample. \n",
    "\n",
    "The confidence interval will be centered on our sample mean. To construct the endpoints we step out from the center based on the amount of variance allowed by our confidence level.\n",
    "\n",
    "We decide our confidence level - do we want to be 80% confident? 90%? 95%? 99%? \n",
    "\n",
    "Based on what we choose, we can then figure out our **margin of error**.\n",
    "\n",
    "### Margin of Error\n",
    "\n",
    "> ### Margin of Error = Critical Value * Sample Standard Error\n",
    "\n",
    "Let's break down those two component parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Value\n",
    "\n",
    "First things first - need your alpha ($\\alpha$). This is what you set when you pick your confidence level!\n",
    "\n",
    "$\\alpha$ = 1 - Confidence Level\n",
    "\n",
    "So, if you pick a 95% confidence level, then $\\alpha$ = 1 - .95 = .05\n",
    "\n",
    "BUT because you want to be confident on either side, this actually ends up being divided by 2! .05 / 2 = **.025** This is the percentage of \"acceptable\" error on either side that you're potentially off by.\n",
    "\n",
    "Why does this matter? Because you'll feed this value into your search for your true critical value - a value which comes from the cumulative probability up until that point (the point at which theres 2.5% on each of the other sides)\n",
    "\n",
    "If this is all really vague, just know it's because I don't want to introduce t-tables or z-tables to you just yet - we'll talk about that SO MUCH MORE in the next session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Error\n",
    "\n",
    "The standard error is the standard deviation of the sampling distribution. The issue is that a sample is not an exact replica of the population. We need to account for that in order to make our estimate of the $\\mu$ value possible. \n",
    "\n",
    "Let's break it down:\n",
    "\n",
    "**Population standard deviation**\n",
    "\n",
    "### $\\sigma _{x} = \\frac{\\sigma }{\\sqrt{n}}$\n",
    "\n",
    "* $ \\sigma _{x}$ = standard error of $\\bar{x} $\n",
    "* $ \\sigma $ = standard deviation of population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What if we do not know the population standard deviation?** (which is most of the time)\n",
    "\n",
    "If we do not know the population standard deviation, we can approximate for it with the sample standard deviation, balanced by the sample size\n",
    "\n",
    "### $\\sigma _{x} â‰ˆ \\frac{s}{\\sqrt{n}}$\n",
    "\n",
    "- s = sample standard deviation\n",
    "- n = sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap it up... in Python!\n",
    "\n",
    "Now that we know the pieces, what we need to calculate in order to understand these pieces, etc - we can just do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out our population parameters\n",
    "pop_std = np.std(df['Hourly Rate'])\n",
    "\n",
    "print(f\"Population Mean: {pop_mean:.2f}, Population Standard Deviation: {pop_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a sample, construct a confidence interval for our sample statistic, and compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a single sample\n",
    "sample = df.sample(n=n) # Remember, n = 50 employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean = np.mean(sample['Hourly Rate'])\n",
    "sample_std = np.std(sample['Hourly Rate'])\n",
    "\n",
    "# We can look at these now... but let's explore how much wiggle room we should give"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Aside: T-Distribution vs. Normal\n",
    "\n",
    "![z vs t](../images/z_vs_t.png)\n",
    "\n",
    "If data is mostly normally distributed, we use the true Gaussian normal distribution when: \n",
    "\n",
    "- n > 100\n",
    "- population standard deviation is known\n",
    "\n",
    "Otherwise, we use the Student's T-Distribution, which has longer tails than a true Gaussian distribution and which accounts for sample size - but, as sample size increases, it looks closer and closer like the true normal distribution.\n",
    "\n",
    "For our purposes, although we know our 'population' standard deviation, let's use the Student's T since we have less than 50 in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats library! Calculating our critical value up until our 2.5%\n",
    "\n",
    "t_value = stats.t.ppf(0.975, n-1) # N-1 shows the 'degrees of freedom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_standard_error = sample_std / (n ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_error = t_value * samp_standard_error\n",
    "\n",
    "interval_min = sample_mean - margin_error\n",
    "interval_max = sample_mean + margin_error\n",
    "print(f\"95% confident our population mean is between {interval_min:.2f} and {interval_max:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hlines(1, interval_min, interval_max, color='r', label='Sample Confidence Interval')\n",
    "plt.vlines(pop_mean, 0, 2, lw=2)\n",
    "plt.xlim(pop_mean-15, pop_mean+15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret: what does this show us?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do all this for 10 samples!\n",
    "\n",
    "intervals = []\n",
    "\n",
    "for x in range(10):\n",
    "    samp = df.sample(n=n) # Remember, n = 50 employees\n",
    "    samp_mean = np.mean(samp['Hourly Rate'])\n",
    "    samp_std = np.std(samp['Hourly Rate'])\n",
    "    \n",
    "    samp_sterr = samp_std / (n ** 0.5)\n",
    "    margin_error = stats.t.ppf(0.975, n-1) * samp_sterr\n",
    "    int_min = samp_mean - margin_error\n",
    "    int_max = samp_mean + margin_error\n",
    "    intervals.append([int_min, int_max])\n",
    "    \n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for x in range(10):\n",
    "    plt.hlines(x+1, intervals[x][0], intervals[x][1],  \n",
    "               label=f'Sample Confidence Interval #{x+1}')\n",
    "plt.vlines(pop_mean, 0, 11, lw=2)\n",
    "plt.xlim(pop_mean-15, pop_mean+15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret - how'd we do?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervals with the Stats Library!\n",
    "\n",
    "AKA - how do we use the `stats` library to calculate the confidence interval for us?\n",
    "\n",
    "The interval function is the easiest to use to calculate these, as it will do the math for you assuming you want the interval to account for both tails etc.\n",
    "\n",
    "Needs:\n",
    "\n",
    "- Confidence Level\n",
    "- Degrees of freedom (n-1)\n",
    "- Sample Mean\n",
    "- Sample Standard Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we need the sample standard error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_se = stats.sem(sample['Hourly Rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, Interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats.t.interval(.95, n-1, sample_mean, stats_se)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
